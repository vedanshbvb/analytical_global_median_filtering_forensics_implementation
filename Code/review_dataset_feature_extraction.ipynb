{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f57u5MWzc8fU",
        "outputId": "ae16f9c3-c00a-4694-99d1-9efb9c94ac3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy scipy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"flamense160/ucid-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "WS6_7rOPfciM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRtENM9r2y71",
        "outputId": "cb59705d-38a7-46f7-a31d-a5a5794e8aed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "dest_folder = '/content/drive/My Drive/ucid/'\n",
        "os.makedirs(dest_folder, exist_ok=True)\n",
        "\n",
        "for root, _, files in os.walk(path):\n",
        "    for file in files:\n",
        "        src = os.path.join(root, file)\n",
        "        dst = os.path.join(dest_folder, file)\n",
        "        shutil.copy(src, dst)\n"
      ],
      "metadata": {
        "id": "ioHnR93ThCgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "# --- CONFIG ---\n",
        "UCID_DIR = Path('/content/drive/My Drive/ucid')\n",
        "OUT_ROOT = Path('/content/drive/My Drive/ucid_dsets')\n",
        "\n",
        "assert UCID_DIR.exists(), f\"{UCID_DIR} not found\"\n",
        "\n",
        "# the “attacks” we’ll do\n",
        "ATTACKS = {\n",
        "    'mf3': lambda img: cv2.medianBlur(img, 3),\n",
        "    'mf5': lambda img: cv2.medianBlur(img, 5),\n",
        "    'avg': lambda img: cv2.blur(img, (3,3)),\n",
        "    'gau': lambda img: cv2.GaussianBlur(img, (3,3), 0.5),\n",
        "    'res': lambda img: cv2.resize(\n",
        "        img,\n",
        "        (int(img.shape[1]*1.5), int(img.shape[0]*1.5)),\n",
        "        interpolation=cv2.INTER_CUBIC\n",
        "    ),\n",
        "    'jpeg': None,  # handled specially\n",
        "    'Orig': None   # just copy/rescale\n",
        "}\n",
        "\n",
        "# target resolutions\n",
        "SIZES = {\n",
        "    '512x384': (512, 384),\n",
        "    '256x256': (256, 256)\n",
        "}\n",
        "\n",
        "# compression variants\n",
        "VARIANTS = {\n",
        "    'unc': ('png', {'compress_level':0}),   # PNG lossless\n",
        "    '90': ('jpg', {'quality':90})           # JPEG Q=90\n",
        "}\n",
        "\n",
        "# Build folder structure\n",
        "for atk in ATTACKS:\n",
        "    for size in SIZES:\n",
        "        for var in VARIANTS:\n",
        "            (OUT_ROOT/atk/f\"{size}_{atk}_{var}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# process images\n",
        "img_exts = {'.png','.jpg','.jpeg','.bmp','.tif','.tiff'}\n",
        "for img_file in sorted(os.listdir(UCID_DIR)):\n",
        "    if Path(img_file).suffix.lower() not in img_exts:\n",
        "        continue\n",
        "    stem = Path(img_file).stem\n",
        "    img_path = UCID_DIR/img_file\n",
        "\n",
        "    # load original in color\n",
        "    orig = cv2.imread(str(img_path))\n",
        "    if orig is None:\n",
        "        print(\"☢️ failed to load\", img_file)\n",
        "        continue\n",
        "\n",
        "    for atk, func in ATTACKS.items():\n",
        "        # 1) get attacked image\n",
        "        if atk in ('jpeg','Orig'):\n",
        "            attacked = orig.copy()\n",
        "        else:\n",
        "            attacked = func(orig)\n",
        "\n",
        "        # 2) for each target size\n",
        "        for size_key, (W,H) in SIZES.items():\n",
        "            resized = cv2.resize(attacked, (W,H), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "            # 3a) save uncompressed PNG\n",
        "            out_dir = OUT_ROOT/atk/f\"{size_key}_{atk}_unc\"\n",
        "            out_path = out_dir/f\"{stem}.png\"\n",
        "            cv2.imwrite(str(out_path), resized,\n",
        "                        [cv2.IMWRITE_PNG_COMPRESSION, 0])\n",
        "\n",
        "            # 3b) save JPEG Q=90\n",
        "            out_dir2 = OUT_ROOT/atk/f\"{size_key}_{atk}_90\"\n",
        "            ext = '.jpg'\n",
        "            jp = out_dir2/f\"{stem}{ext}\"\n",
        "            # use PIL to control Q\n",
        "            rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n",
        "            Image.fromarray(rgb).save(str(jp), 'JPEG', quality=90)\n",
        "\n",
        "print(\"✅ Dataset preparation complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml1iOERe7wYA",
        "outputId": "c515afb1-84ba-4816-9044-eb24a9c81fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset preparation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# working code"
      ],
      "metadata": {
        "id": "d9DduipE67fP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "from skimage.util import view_as_windows\n",
        "\n",
        "def cal_o_n_mf_ovrblk_moments(X, Xmf, win_sz, px_ol):\n",
        "\n",
        "    def pad_to_odd(im):\n",
        "        h, w = im.shape\n",
        "        if h % 2 == 0:\n",
        "            im = np.vstack([im, np.zeros((1, w))])\n",
        "        if w % 2 == 0:\n",
        "            im = np.hstack([im, np.zeros((im.shape[0], 1))])\n",
        "        return im\n",
        "\n",
        "    if px_ol == 1:\n",
        "        X = pad_to_odd(X)\n",
        "        Xmf = pad_to_odd(Xmf)\n",
        "        step = win_sz - 1\n",
        "    elif px_ol == 2:\n",
        "        step = win_sz - 2\n",
        "        # don't pad for 2-px overlap\n",
        "\n",
        "    #overlapping blocks using view_as_windows\n",
        "    blocks_o = view_as_windows(X, (win_sz, win_sz), step)\n",
        "    blocks_mf = view_as_windows(Xmf, (win_sz, win_sz), step)\n",
        "\n",
        "    n_blocks = blocks_o.shape[0] * blocks_o.shape[1]\n",
        "    blocks_o_flat = blocks_o.reshape(n_blocks, -1)\n",
        "    blocks_mf_flat = blocks_mf.reshape(n_blocks, -1)\n",
        "\n",
        "    #skewness and kurtosis in batch\n",
        "    skew_o = stats.skew(blocks_o_flat, axis=1, nan_policy='omit')\n",
        "    skew_mf = stats.skew(blocks_mf_flat, axis=1, nan_policy='omit')\n",
        "    kurt_o = stats.kurtosis(blocks_o_flat, axis=1, nan_policy='omit')\n",
        "    kurt_mf = stats.kurtosis(blocks_mf_flat, axis=1, nan_policy='omit')\n",
        "\n",
        "    valid_skew = ~np.isnan(skew_o) & ~np.isnan(skew_mf)\n",
        "    valid_kurt = ~np.isnan(kurt_o) & ~np.isnan(kurt_mf)\n",
        "\n",
        "    #count NaNs\n",
        "    n_nanskewo = np.isnan(skew_o).sum()\n",
        "    n_nanskewmf = np.isnan(skew_mf).sum()\n",
        "    n_nankurto = np.isnan(kurt_o).sum()\n",
        "    n_nankurtmf = np.isnan(kurt_mf).sum()\n",
        "\n",
        "    if np.all(np.isnan(skew_o)) or np.all(np.isnan(skew_mf)):\n",
        "        skeworemnan_ovblk = skewmfremnan_ovblk = np.ones(n_blocks)\n",
        "        n_ovblk_remnanskew = 0\n",
        "        chk_skew = 1\n",
        "    else:\n",
        "        skeworemnan_ovblk = skew_o[valid_skew]\n",
        "        skewmfremnan_ovblk = skew_mf[valid_skew]\n",
        "        n_ovblk_remnanskew = len(skeworemnan_ovblk)\n",
        "        chk_skew = 0\n",
        "\n",
        "    if np.all(np.isnan(kurt_o)) or np.all(np.isnan(kurt_mf)):\n",
        "        kurtoremnan_ovblk = kurtmfremnan_ovblk = np.ones(n_blocks)\n",
        "        n_ovblk_remnankurt = 0\n",
        "        chk_kurt = 1\n",
        "    else:\n",
        "        kurtoremnan_ovblk = kurt_o[valid_kurt]\n",
        "        kurtmfremnan_ovblk = kurt_mf[valid_kurt]\n",
        "        n_ovblk_remnankurt = len(kurtoremnan_ovblk)\n",
        "        chk_kurt = 0\n",
        "\n",
        "    return (skeworemnan_ovblk, skewmfremnan_ovblk,\n",
        "            kurtoremnan_ovblk, kurtmfremnan_ovblk,\n",
        "            n_ovblk_remnanskew, n_ovblk_remnankurt,\n",
        "            n_nanskewo, n_nanskewmf, n_nankurto, n_nankurtmf,\n",
        "            chk_skew, chk_kurt)\n"
      ],
      "metadata": {
        "id": "s8OELjnp7Bys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "all attacks"
      ],
      "metadata": {
        "id": "kYVhzI_lCU_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from scipy import stats\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# Dataset structure parameters\n",
        "OUT_ROOT = Path('/content/drive/My Drive/ucid_dsets')  # Your dataset root\n",
        "ATTACKS = ['mf3', 'mf5', 'avg', 'gau', 'res', 'jpeg', 'orig']\n",
        "SIZES = ['512x384', '256x256']\n",
        "VARIANTS = ['unc', '90']\n",
        "UCID_FEAT_DIR = Path('/content/drive/My Drive/ucid_dsets/ucid_features')\n",
        "UCID_FEAT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# Algorithm parameters\n",
        "win_sz = 3\n",
        "px_ol = 2\n",
        "\n",
        "# Process each attack type, size, and variant\n",
        "for attack in ATTACKS:\n",
        "    for size in SIZES:\n",
        "        for variant in VARIANTS:\n",
        "            # Skip some combinations if needed\n",
        "            if attack == 'jpeg' and variant == 'unc':\n",
        "                continue  # Skip jpeg with unc variant as it doesn't exist\n",
        "\n",
        "            # Define directories\n",
        "            attack_dir = OUT_ROOT / attack / f\"{size}_{attack}_{variant}\"\n",
        "            original_dir = OUT_ROOT / 'orig' / f\"{size}_orig_{'unc' if variant == 'unc' else '90'}\"\n",
        "\n",
        "            # Check if directories exist\n",
        "            if not attack_dir.exists() or not original_dir.exists():\n",
        "                print(f\"Skipping {attack_dir} or {original_dir} - directory not found\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Processing {attack} {size} {variant}...\")\n",
        "\n",
        "            # Get list of images\n",
        "            image_extensions = ['.jpg', '.jpeg', '.png', '.tif', '.tiff', '.bmp']\n",
        "            attack_files = [f for f in os.listdir(attack_dir) if os.path.splitext(f.lower())[1] in image_extensions]\n",
        "            original_files = [f for f in os.listdir(original_dir) if os.path.splitext(f.lower())[1] in image_extensions]\n",
        "\n",
        "            # Ensure the files match\n",
        "            attack_stems = [Path(f).stem for f in attack_files]\n",
        "            original_stems = [Path(f).stem for f in original_files]\n",
        "            common_stems = set(attack_stems).intersection(set(original_stems))\n",
        "\n",
        "            if not common_stems:\n",
        "                print(f\"No matching files found between {attack_dir} and {original_dir}\")\n",
        "                continue\n",
        "\n",
        "            # Filter files to include only common stems\n",
        "            attack_files = [f for f in attack_files if Path(f).stem in common_stems]\n",
        "            original_files = [f for f in original_files if Path(f).stem in common_stems]\n",
        "\n",
        "            # Sort files to ensure matching\n",
        "            attack_files.sort()\n",
        "            original_files.sort()\n",
        "\n",
        "            count = len(attack_files)\n",
        "            if count == 0:\n",
        "                print(f\"No files found in {attack_dir}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Found {count} matching files\")\n",
        "\n",
        "            # Initialize feature arrays for attacked images only\n",
        "            skr_4pk = np.zeros(count)\n",
        "            skl_4pk = np.zeros(count)\n",
        "            mean_sk = np.zeros(count)\n",
        "            var_sk = np.zeros(count)\n",
        "            kurt = np.zeros(count)\n",
        "            mid_pk = np.zeros(count)\n",
        "            skr_1pk = np.zeros(count)\n",
        "            skl_1pk = np.zeros(count)\n",
        "            skr_2pk = np.zeros(count)\n",
        "            skl_2pk = np.zeros(count)\n",
        "            skr_3pk = np.zeros(count)\n",
        "            skl_3pk = np.zeros(count)\n",
        "            n_nansk = np.zeros(count)\n",
        "            ku_1pk = np.zeros(count)\n",
        "            ku_2pk = np.zeros(count)\n",
        "            ku_3pk = np.zeros(count)\n",
        "            ku_4pk = np.zeros(count)\n",
        "            mean_ku = np.zeros(count)\n",
        "            var_ku = np.zeros(count)\n",
        "\n",
        "            # Process each image\n",
        "            for t in tqdm(range(count), desc=f\"Processing {attack} {size} {variant}\"):\n",
        "                # Load original image\n",
        "                orig_path = original_dir / original_files[t]\n",
        "                orig_img = cv2.imread(str(orig_path), cv2.IMREAD_UNCHANGED)\n",
        "                if orig_img is None:\n",
        "                    print(f\"Failed to load original image: {orig_path}\")\n",
        "                    continue\n",
        "\n",
        "                # Convert to grayscale if needed\n",
        "                if len(orig_img.shape) == 3:\n",
        "                    orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2GRAY)\n",
        "                orig_img = orig_img.astype(float)\n",
        "\n",
        "                # Load attacked image\n",
        "                attack_path = attack_dir / attack_files[t]\n",
        "                attack_img = cv2.imread(str(attack_path), cv2.IMREAD_UNCHANGED)\n",
        "                if attack_img is None:\n",
        "                    print(f\"Failed to load attacked image: {attack_path}\")\n",
        "                    continue\n",
        "\n",
        "                # Convert to grayscale if needed\n",
        "                if len(attack_img.shape) == 3:\n",
        "                    attack_img = cv2.cvtColor(attack_img, cv2.COLOR_BGR2GRAY)\n",
        "                attack_img = attack_img.astype(float)\n",
        "\n",
        "                # Calculate moments\n",
        "                (skeworemnan3x3o, skewmf3x3remnan3x3o, kurtoremnan3x3o, kurtmf3x3remnan3x3o,\n",
        "                 novblk_remnanskew, novblk_remnankurt, n_nanskewo, n_nanskewmf, n_nankurto,\n",
        "                 n_nankurtmf, chk_skew, chk_kurt) = cal_o_n_mf_ovrblk_moments(orig_img, attack_img, win_sz, px_ol)\n",
        "\n",
        "                # Handle invalid cases\n",
        "                if chk_skew == 1:\n",
        "                    mean_sk[t] = 0\n",
        "                    var_sk[t] = 0\n",
        "                    kurt[t] = 0\n",
        "                    mid_pk[t] = 0\n",
        "                    skr_1pk[t] = 0\n",
        "                    skl_1pk[t] = 0\n",
        "                    skr_2pk[t] = 0\n",
        "                    skl_2pk[t] = 0\n",
        "                    skr_3pk[t] = 0\n",
        "                    skl_3pk[t] = 0\n",
        "                    skl_4pk[t] = 0\n",
        "                    skr_4pk[t] = 0\n",
        "                    n_nansk[t] = n_nanskewmf\n",
        "\n",
        "                if chk_kurt == 1:\n",
        "                    ku_1pk[t] = 0\n",
        "                    ku_2pk[t] = 0\n",
        "                    ku_3pk[t] = 0\n",
        "                    ku_4pk[t] = 0\n",
        "                    mean_ku[t] = 0\n",
        "                    var_ku[t] = 0\n",
        "\n",
        "                if chk_skew == 0 and chk_kurt == 0:\n",
        "                    # Calculate number of bins for histogram\n",
        "                    n_binskew = 1 + np.ceil(np.log2(novblk_remnanskew))\n",
        "\n",
        "                    sigma = np.sqrt((6 * (novblk_remnankurt - 2) / (novblk_remnankurt + 1) * (novblk_remnankurt + 3)))\n",
        "                    n_binkurt = 1 + np.ceil(np.log2(novblk_remnankurt) + np.log2(1 + (abs(stats.skew(kurtoremnan3x3o))) / sigma))\n",
        "\n",
        "                    if np.isnan(n_binkurt):\n",
        "                        n_binkurt = n_binskew\n",
        "\n",
        "                    # Calculate histograms for attacked image features\n",
        "                    n2, x2 = np.histogram(skewmf3x3remnan3x3o, int(n_binskew))\n",
        "                    n4, x4 = np.histogram(kurtmf3x3remnan3x3o, int(n_binkurt))\n",
        "\n",
        "                    # Convert bin edges to bin centers\n",
        "                    x2 = (x2[:-1] + x2[1:]) / 2\n",
        "                    x4 = (x4[:-1] + x4[1:]) / 2\n",
        "\n",
        "                    # Calculate bin widths\n",
        "                    if n_binskew == 1:\n",
        "                        _, edges2 = np.histogram(skewmf3x3remnan3x3o, 1)\n",
        "                        bin_wskmf = edges2[1] - edges2[0]\n",
        "                    else:\n",
        "                        bin_wskmf = x2[1] - x2[0] if len(x2) > 1 else 0\n",
        "\n",
        "                    if n_binkurt == 1:\n",
        "                        _, edges4 = np.histogram(kurtmf3x3remnan3x3o, 1)\n",
        "                        bin_wkumf = edges4[1] - edges4[0]\n",
        "                    else:\n",
        "                        bin_wkumf = x4[1] - x4[0] if len(x4) > 1 else 0\n",
        "\n",
        "                    # Calculate features\n",
        "                    mean_sk[t] = np.mean(skewmf3x3remnan3x3o)\n",
        "                    var_sk[t] = np.var(skewmf3x3remnan3x3o)\n",
        "                    kurt[t] = stats.kurtosis(skewmf3x3remnan3x3o)\n",
        "\n",
        "                    # Find specific skewness and kurtosis values\n",
        "                    # Feature - check for skewness value (2.4748)\n",
        "                    for i in range(len(x2)):\n",
        "                        x2_upper = np.fix((x2[i] + bin_wskmf / 2) * (10**4)) / (10**4)\n",
        "                        x2_lower = np.fix((x2[i] - bin_wskmf / 2) * (10**4)) / (10**4)\n",
        "                        if x2_upper >= 2.4748 and x2_lower <= 2.4748:\n",
        "                            skr_4pk[t] = n2[i]\n",
        "\n",
        "                    # Feature - check for skewness value (-2.4748)\n",
        "                    for i in range(len(x2)):\n",
        "                        x2_upper = np.fix((x2[i] + bin_wskmf / 2) * (10**4)) / (10**4)\n",
        "                        x2_lower = np.fix((x2[i] - bin_wskmf / 2) * (10**4)) / (10**4)\n",
        "                        if x2_upper >= -2.4748 and x2_lower <= -2.4748:\n",
        "                            skl_4pk[t] = n2[i]\n",
        "\n",
        "                    # Feature - mid point check\n",
        "                    for i in range(len(x2)):\n",
        "                        x2_upper = np.fix((x2[i] + bin_wskmf / 2) * (10**4)) / (10**4)\n",
        "                        x2_lower = np.fix((x2[i] - bin_wskmf / 2) * (10**4)) / (10**4)\n",
        "                        if x2_upper >= 0.0000 and x2_lower <= 0.0000:\n",
        "                            mid_pk[t] = n2[i]\n",
        "\n",
        "                    # Feature - check for skewness value (1.3363)\n",
        "                    for i in range(len(x2)):\n",
        "                        x2_upper = np.fix((x2[i] + bin_wskmf / 2) * (10**4)) / (10**4)\n",
        "                        x2_lower = np.fix((x2[i] - bin_wskmf / 2) * (10**4)) / (10**4)\n",
        "                        if x2_upper >= 1.3363 and x2_lower <= 1.3363:\n",
        "                            skr_1pk[t] = n2[i]\n",
        "\n",
        "                    # Feature - check for skewness value (-1.3363)\n",
        "                    for i in range(len(x2)):\n",
        "                        x2_upper = np.fix((x2[i] + bin_wskmf / 2) * (10**4)) / (10**4)\n",
        "                        x2_lower = np.fix((x2[i] - bin_wskmf / 2) * (10**4)) / (10**4)\n",
        "                        if x2_upper >= -1.3363 and x2_lower <= -1.3363:\n",
        "                            skl_1pk[t] = n2[i]\n",
        "\n",
        "                    # Feature - check for skewness value (0.7071)\n",
        "                    for i in range(len(x2)):\n",
        "                        x2_upper = np.fix((x2[i] + bin_wskmf / 2) * (10**4)) / (10**4)\n",
        "                        x2_lower = np.fix((x2[i] - bin_wskmf / 2) * (10**4)) / (10**4)\n",
        "                        if x2_upper >= 0.7071 and x2_lower <= 0.7071:\n",
        "                            skr_2pk[t] = n2[i]\n",
        "\n",
        "                    # Feature - check for skewness value (-0.7071)\n",
        "                    for i in range(len(x2)):\n",
        "                        x2_upper = np.fix((x2[i] + bin_wskmf / 2) * (10**4)) / (10**4)\n",
        "                        x2_lower = np.fix((x2[i] - bin_wskmf / 2) * (10**4)) / (10**4)\n",
        "                        if x2_upper >= -0.7071 and x2_lower <= -0.7071:\n",
        "                            skl_2pk[t] = n2[i]\n",
        "\n",
        "                    # Feature - check for skewness value (0.2236)\n",
        "                    for i in range(len(x2)):\n",
        "                        x2_upper = np.fix((x2[i] + bin_wskmf / 2) * (10**4)) / (10**4)\n",
        "                        x2_lower = np.fix((x2[i] - bin_wskmf / 2) * (10**4)) / (10**4)\n",
        "                        if x2_upper >= 0.2236 and x2_lower <= 0.2236:\n",
        "                            skr_3pk[t] = n2[i]\n",
        "\n",
        "                    # Feature - check for skewness value (-0.2236)\n",
        "                    for i in range(len(x2)):\n",
        "                        x2_upper = np.fix((x2[i] + bin_wskmf / 2) * (10**4)) / (10**4)\n",
        "                        x2_lower = np.fix((x2[i] - bin_wskmf / 2) * (10**4)) / (10**4)\n",
        "                        if x2_upper >= -0.2236 and x2_lower <= -0.2236:\n",
        "                            skl_3pk[t] = n2[i]\n",
        "\n",
        "                    # Feature - nan counts\n",
        "                    n_nansk[t] = n_nanskewmf\n",
        "\n",
        "                    # Feature - check for kurtosis value (2.7857)\n",
        "                    for i in range(len(x4)):\n",
        "                        x4_upper = np.fix((x4[i] + bin_wkumf / 2) * (10**4)) / (10**4)\n",
        "                        x4_lower = np.fix((x4[i] - bin_wkumf / 2) * (10**4)) / (10**4)\n",
        "                        if x4_upper >= 2.7857 and x4_lower <= 2.7857:\n",
        "                            ku_1pk[t] = n4[i]\n",
        "\n",
        "                    # Feature - check for kurtosis value (1.5000)\n",
        "                    for i in range(len(x4)):\n",
        "                        x4_upper = np.fix((x4[i] + bin_wkumf / 2) * (10**4)) / (10**4)\n",
        "                        x4_lower = np.fix((x4[i] - bin_wkumf / 2) * (10**4)) / (10**4)\n",
        "                        if x4_upper >= 1.5000 and x4_lower <= 1.5000:\n",
        "                            ku_2pk[t] = n4[i]\n",
        "\n",
        "                    # Feature - check for kurtosis value (1.0500)\n",
        "                    for i in range(len(x4)):\n",
        "                        x4_upper = np.fix((x4[i] + bin_wkumf / 2) * (10**4)) / (10**4)\n",
        "                        x4_lower = np.fix((x4[i] - bin_wkumf / 2) * (10**4)) / (10**4)\n",
        "                        if x4_upper >= 1.0500 and x4_lower <= 1.0500:\n",
        "                            ku_3pk[t] = n4[i]\n",
        "\n",
        "                    # Feature - check for kurtosis value (7.1249)\n",
        "                    for i in range(len(x4)):\n",
        "                        x4_upper = np.fix((x4[i] + bin_wkumf / 2) * (10**4)) / (10**4)\n",
        "                        x4_lower = np.fix((x4[i] - bin_wkumf / 2) * (10**4)) / (10**4)\n",
        "                        if x4_upper >= 7.1249 and x4_lower <= 7.1249:\n",
        "                            ku_4pk[t] = n4[i]\n",
        "\n",
        "                    # Feature - mean and variance of kurtosis\n",
        "                    mean_ku[t] = np.mean(kurtmf3x3remnan3x3o)\n",
        "                    var_ku[t] = np.var(kurtmf3x3remnan3x3o)\n",
        "\n",
        "            # Create feature matrix for attacked images only\n",
        "            features_raw = np.column_stack([\n",
        "                skl_4pk, skr_4pk, mean_sk, var_sk, kurt, mid_pk, skr_1pk, skl_1pk,\n",
        "                skr_2pk, skl_2pk, skr_3pk, skl_3pk, n_nansk, ku_1pk, ku_2pk,\n",
        "                ku_3pk, ku_4pk, mean_ku, var_ku\n",
        "            ])\n",
        "\n",
        "            # Calculate min and max for normalization\n",
        "            min_vals = np.min(features_raw, axis=0)\n",
        "            max_vals = np.max(features_raw, axis=0)\n",
        "\n",
        "            # Create normalized features\n",
        "            features_normalized = features_raw.copy()\n",
        "            for j in range(count):\n",
        "                for k in range(features_raw.shape[1]):\n",
        "                    if max_vals[k] != min_vals[k]:\n",
        "                        features_normalized[j, k] = ((2 * (features_raw[j, k] - min_vals[k])) /\n",
        "                                                   (max_vals[k] - min_vals[k])) - 1\n",
        "            # ... inside your attack/size/variant loops, after building feat_arr:\n",
        "\n",
        "            # Save results\n",
        "            # output_name = f\"{size}_{attack}_{variant}\"\n",
        "            out_name = f\"{size}_{attack}_{variant}.npy\"\n",
        "            out_path = UCID_FEAT_DIR / out_name\n",
        "            np.save(out_path, features_normalized)\n",
        "\n",
        "            print(f\"Saved features for {out_path}\")\n",
        "\n",
        "print(\"✅ Feature extraction complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAP9i3IzCUu4",
        "outputId": "b4b28a86-96c3-4222-d458-e5aaa2464f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing mf3 512x384 unc...\n",
            "Found 1338 matching files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing mf3 512x384 unc:   0%|          | 0/1338 [00:00<?, ?it/s]<ipython-input-11-a342e33a06b3>:32: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
            "  skew_o = stats.skew(blocks_o_flat, axis=1, nan_policy='omit')\n",
            "<ipython-input-11-a342e33a06b3>:33: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
            "  skew_mf = stats.skew(blocks_mf_flat, axis=1, nan_policy='omit')\n",
            "<ipython-input-11-a342e33a06b3>:34: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
            "  kurt_o = stats.kurtosis(blocks_o_flat, axis=1, nan_policy='omit')\n",
            "<ipython-input-11-a342e33a06b3>:35: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
            "  kurt_mf = stats.kurtosis(blocks_mf_flat, axis=1, nan_policy='omit')\n",
            "Processing mf3 512x384 unc: 100%|██████████| 1338/1338 [10:45<00:00,  2.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features for 512x384_mf3_unc\n",
            "Processing mf3 512x384 90...\n",
            "Found 1338 matching files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing mf3 512x384 90: 100%|██████████| 1338/1338 [10:19<00:00,  2.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features for 512x384_mf3_90\n",
            "Processing mf3 256x256 unc...\n",
            "Found 1338 matching files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing mf3 256x256 unc: 100%|██████████| 1338/1338 [03:35<00:00,  6.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features for 256x256_mf3_unc\n",
            "Processing mf3 256x256 90...\n",
            "Found 1338 matching files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing mf3 256x256 90: 100%|██████████| 1338/1338 [03:30<00:00,  6.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features for 256x256_mf3_90\n",
            "Processing mf5 512x384 unc...\n",
            "Found 1338 matching files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing mf5 512x384 unc: 100%|██████████| 1338/1338 [10:40<00:00,  2.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features for 512x384_mf5_unc\n",
            "Processing mf5 512x384 90...\n",
            "Found 1338 matching files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing mf5 512x384 90: 100%|██████████| 1338/1338 [10:28<00:00,  2.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features for 512x384_mf5_90\n",
            "Processing mf5 256x256 unc...\n",
            "Found 1338 matching files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing mf5 256x256 unc:  53%|█████▎    | 709/1338 [02:06<01:33,  6.74it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# creating MF35"
      ],
      "metadata": {
        "id": "f-TgC306-RxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def create_mf35_dataset(mf3_files, mf5_files, output_dir):\n",
        "    \"\"\"\n",
        "    Create mf35 dataset by randomly selecting 50% of images from mf3 and mf5 datasets.\n",
        "    Each file contains data with shape (1338, 19) where 1338 is the number of images\n",
        "    and 19 is the number of features per image.\n",
        "\n",
        "    Args:\n",
        "        mf3_files: List of paths to mf3 .npy files\n",
        "        mf5_files: List of paths to mf5 .npy files\n",
        "        output_dir: Directory to save mf35 files\n",
        "    \"\"\"\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for mf3_file, mf5_file in zip(mf3_files, mf5_files):\n",
        "        mf3_basename = os.path.basename(mf3_file)\n",
        "        # Get the corresponding mf35 filename\n",
        "        mf35_basename = mf3_basename.replace('mf3', 'mf35')\n",
        "        mf35_filepath = os.path.join(output_dir, mf35_basename)\n",
        "\n",
        "        print(f\"Processing {mf3_basename} and {os.path.basename(mf5_file)}...\")\n",
        "\n",
        "        # Load data from both files\n",
        "        mf3_data = np.load(mf3_file)\n",
        "        mf5_data = np.load(mf5_file)\n",
        "\n",
        "        # Check dimensions\n",
        "        if mf3_data.shape != mf5_data.shape:\n",
        "            print(f\"Warning: Shape mismatch between {mf3_file} ({mf3_data.shape}) and {mf5_file} ({mf5_data.shape})\")\n",
        "            continue\n",
        "\n",
        "        # Get total number of images\n",
        "        num_images = mf3_data.shape[0]  # Should be 1338\n",
        "\n",
        "        # Randomly select 50% indices for mf3\n",
        "        np.random.seed(42)  # For reproducibility, remove if not needed\n",
        "        mf3_indices = np.random.choice(num_images, num_images // 2, replace=False)\n",
        "\n",
        "        # Get remaining indices for mf5\n",
        "        all_indices = set(range(num_images))\n",
        "        mf5_indices = list(all_indices - set(mf3_indices))\n",
        "\n",
        "        # Create the merged dataset\n",
        "        mf35_data = np.zeros_like(mf3_data)\n",
        "        mf35_data[mf3_indices] = mf3_data[mf3_indices]  # 50% from mf3\n",
        "        mf35_data[mf5_indices] = mf5_data[mf5_indices]  # 50% from mf5\n",
        "\n",
        "        # Save the new dataset\n",
        "        np.save(mf35_filepath, mf35_data)\n",
        "        print(f\"Created {mf35_filepath} with shape {mf35_data.shape}\")\n",
        "\n",
        "def main():\n",
        "    # Define the dataset files\n",
        "    base_dir = \"/content/Drive/My Drive/ucid_dsets/ucid_features/\"  # Current directory, change if needed\n",
        "\n",
        "    mf3_files = [\n",
        "        os.path.join(base_dir, \"512x384_mf3_unc.npy\"),\n",
        "        os.path.join(base_dir, \"512x384_mf3_90.npy\"),\n",
        "        os.path.join(base_dir, \"256x256_mf3_unc.npy\"),\n",
        "        os.path.join(base_dir, \"256x256_mf3_90.npy\")\n",
        "    ]\n",
        "\n",
        "    mf5_files = [\n",
        "        os.path.join(base_dir, \"512x384_mf5_unc.npy\"),\n",
        "        os.path.join(base_dir, \"512x384_mf5_90.npy\"),\n",
        "        os.path.join(base_dir, \"256x256_mf5_unc.npy\"),\n",
        "        os.path.join(base_dir, \"256x256_mf5_90.npy\")\n",
        "    ]\n",
        "\n",
        "    # Check if files exist before processing\n",
        "    for file_list in [mf3_files, mf5_files]:\n",
        "        for file in file_list:\n",
        "            if not os.path.exists(file):\n",
        "                print(f\"Warning: File {file} does not exist\")\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    output_dir = os.path.join(base_dir, \"mf35\")\n",
        "\n",
        "    # Process and create mf35 dataset\n",
        "    create_mf35_dataset(mf3_files, mf5_files, output_dir)\n",
        "\n",
        "    print(\"mf35 dataset creation complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCWhI6VxRuBR",
        "outputId": "d1fabd5c-003a-4839-c62a-f9e684f42953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 512x384_mf3_unc.npy and 512x384_mf5_unc.npy...\n",
            "Created /content/Drive/My Drive/ucid_dsets/ucid_features/mf35/512x384_mf35_unc.npy with shape (1338, 19)\n",
            "Processing 512x384_mf3_90.npy and 512x384_mf5_90.npy...\n",
            "Created /content/Drive/My Drive/ucid_dsets/ucid_features/mf35/512x384_mf35_90.npy with shape (1338, 19)\n",
            "Processing 256x256_mf3_unc.npy and 256x256_mf5_unc.npy...\n",
            "Created /content/Drive/My Drive/ucid_dsets/ucid_features/mf35/256x256_mf35_unc.npy with shape (1338, 19)\n",
            "Processing 256x256_mf3_90.npy and 256x256_mf5_90.npy...\n",
            "Created /content/Drive/My Drive/ucid_dsets/ucid_features/mf35/256x256_mf35_90.npy with shape (1338, 19)\n",
            "mf35 dataset creation complete!\n"
          ]
        }
      ]
    }
  ]
}