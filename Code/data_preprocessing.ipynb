{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mf3\n",
    "512x384_mf3_unc.npy\n",
    "512x384_mf3_90.npy\n",
    "256x256_mf3_unc.npy\n",
    "256x256_mf3_90.npy\n",
    "Mf5\n",
    "512x384_mf5_unc.npy\n",
    "512x384_mf5_90.npy\n",
    "256x256_mf5_unc.npy\n",
    "256x256_mf5_90.npy\n",
    "Mf35\n",
    "512x384_mf35_unc.npy\n",
    "512x384_mf35_90.npy\n",
    "256x256_mf35_unc.npy\n",
    "256x256_mf35_90.npy\n",
    "avg\n",
    "512x384_avg_unc.npy\n",
    "512x384_avg_90.npy\n",
    "256x256_avg_unc.npy\n",
    "256x256_avg_90.npy\n",
    "gau\n",
    "512x384_gau_unc.npy\n",
    "512x384_gau_90.npy\n",
    "256x256_gau_unc.npy\n",
    "256x256_gau_90.npy\n",
    "res\n",
    "512x384_res_unc.npy\n",
    "512x384_res_90.npy\n",
    "256x256_res_unc.npy\n",
    "256x256_res_90.npy\n",
    "jpeg\n",
    "512x384_jpeg_90.npy\n",
    "256x256_jpeg_90.npy\n",
    "Orig\n",
    "512x384_orig_unc.npy\n",
    "256x256_orig_unc.npy\n",
    "All\n",
    "512x384_all_unc.npy\n",
    "256x256_all_unc.npy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated and saved: 512x384_mf3_unc.npy\n",
      "Updated and saved: 512x384_mf3_90.npy\n",
      "Updated and saved: 256x256_mf3_unc.npy\n",
      "Updated and saved: 256x256_mf3_90.npy\n",
      "Updated and saved: 512x384_mf5_unc.npy\n",
      "Updated and saved: 512x384_mf5_90.npy\n",
      "Updated and saved: 256x256_mf5_unc.npy\n",
      "Updated and saved: 256x256_mf5_90.npy\n",
      "Updated and saved: 512x384_mf35_unc.npy\n",
      "Updated and saved: 512x384_mf35_90.npy\n",
      "Updated and saved: 256x256_mf35_unc.npy\n",
      "Updated and saved: 256x256_mf35_90.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# List the filenames manually\n",
    "file_names = [\n",
    "    \"512x384_mf3_unc.npy\",\n",
    "    \"512x384_mf3_90.npy\",\n",
    "    \"256x256_mf3_unc.npy\",\n",
    "    \"256x256_mf3_90.npy\",\n",
    "    \n",
    "    \"512x384_mf5_unc.npy\",\n",
    "    \"512x384_mf5_90.npy\",\n",
    "    \"256x256_mf5_unc.npy\",\n",
    "    \"256x256_mf5_90.npy\",\n",
    "    \n",
    "    \"512x384_mf35_unc.npy\",\n",
    "    \"512x384_mf35_90.npy\",\n",
    "    \"256x256_mf35_unc.npy\",\n",
    "    \"256x256_mf35_90.npy\"\n",
    "]\n",
    "\n",
    "# Process each file\n",
    "for filename in file_names:\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"File not found: {filename}\")\n",
    "        continue\n",
    "\n",
    "    # Load the data\n",
    "    data = np.load(filename)\n",
    "\n",
    "    # Check if it has 19 columns\n",
    "    if data.shape[1] != 19:\n",
    "        print(f\"Skipping {filename}: expected 19 columns, found {data.shape[1]}\")\n",
    "        continue\n",
    "\n",
    "    # Create and append a column of ones\n",
    "    ones_column = np.ones((data.shape[0], 1))\n",
    "    data_with_label = np.hstack((data, ones_column))\n",
    "\n",
    "    # Overwrite the original file\n",
    "    np.save(filename, data_with_label)\n",
    "\n",
    "    print(f\"Updated and saved: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated and saved: 512x384_avg_unc.npy\n",
      "Updated and saved: 512x384_avg_90.npy\n",
      "Updated and saved: 256x256_avg_unc.npy\n",
      "Updated and saved: 256x256_avg_90.npy\n",
      "Updated and saved: 512x384_gau_unc.npy\n",
      "Updated and saved: 512x384_gau_90.npy\n",
      "Updated and saved: 256x256_gau_unc.npy\n",
      "Updated and saved: 256x256_gau_90.npy\n",
      "Updated and saved: 512x384_res_unc.npy\n",
      "Updated and saved: 512x384_res_90.npy\n",
      "Updated and saved: 256x256_res_unc.npy\n",
      "Updated and saved: 256x256_res_90.npy\n",
      "Updated and saved: 512x384_jpeg_90.npy\n",
      "Updated and saved: 256x256_jpeg_90.npy\n",
      "Updated and saved: 512x384_orig_unc.npy\n",
      "Updated and saved: 256x256_orig_unc.npy\n",
      "File not found: 512x384_all_unc.npy\n",
      "File not found: 256x256_all_unc.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# List the filenames manually (with quotes and commas)\n",
    "file_names = [\n",
    "    \"512x384_avg_unc.npy\",\n",
    "    \"512x384_avg_90.npy\",\n",
    "    \"256x256_avg_unc.npy\",\n",
    "    \"256x256_avg_90.npy\",\n",
    "\n",
    "    \"512x384_gau_unc.npy\",\n",
    "    \"512x384_gau_90.npy\",\n",
    "    \"256x256_gau_unc.npy\",\n",
    "    \"256x256_gau_90.npy\",\n",
    "\n",
    "    \"512x384_res_unc.npy\",\n",
    "    \"512x384_res_90.npy\",\n",
    "    \"256x256_res_unc.npy\",\n",
    "    \"256x256_res_90.npy\",\n",
    "\n",
    "    \"512x384_jpeg_90.npy\",\n",
    "    \"256x256_jpeg_90.npy\",\n",
    "\n",
    "    \"512x384_orig_unc.npy\",\n",
    "    \"256x256_orig_unc.npy\",\n",
    "\n",
    "    \"512x384_all_unc.npy\",\n",
    "    \"256x256_all_unc.npy\"\n",
    "]\n",
    "\n",
    "# Process each file\n",
    "for filename in file_names:\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"File not found: {filename}\")\n",
    "        continue\n",
    "\n",
    "    # Load the data\n",
    "    data = np.load(filename)\n",
    "\n",
    "    # Check if it has 19 columns\n",
    "    if data.shape[1] != 19:\n",
    "        print(f\"Skipping {filename}: expected 19 columns, found {data.shape[1]}\")\n",
    "        continue\n",
    "\n",
    "    # Create and append a column of zeros\n",
    "    zeros_column = np.zeros((data.shape[0], 1))\n",
    "    data_with_label = np.hstack((data, zeros_column))\n",
    "\n",
    "    # Overwrite the original file\n",
    "    np.save(filename, data_with_label)\n",
    "\n",
    "    print(f\"Updated and saved: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (1336, 20)\n",
      "Saved to '512x384_all_unc.npy'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# List of input file names\n",
    "file_names = [\n",
    "    '512x384_avg_unc.npy',\n",
    "    '512x384_gau_unc.npy',\n",
    "    '512x384_jpeg_90.npy',\n",
    "    '512x384_res_unc.npy'\n",
    "]\n",
    "\n",
    "# Container for sampled data\n",
    "sampled_data = []\n",
    "\n",
    "# Process each file\n",
    "for file in file_names:\n",
    "    if not os.path.exists(file):\n",
    "        raise FileNotFoundError(f\"File not found: {file}\")\n",
    "    \n",
    "    data = np.load(file)\n",
    "    total_samples = data.shape[0]\n",
    "    sample_size = total_samples // 4  # 25%\n",
    "    \n",
    "    # Randomly select 25% of the data\n",
    "    indices = np.random.choice(total_samples, sample_size, replace=False)\n",
    "    sampled = data[indices]\n",
    "    \n",
    "    sampled_data.append(sampled)\n",
    "\n",
    "# Concatenate all sampled data\n",
    "all_sampled_data = np.concatenate(sampled_data, axis=0)\n",
    "\n",
    "# Save to new file\n",
    "np.save('512x384_all_unc.npy', all_sampled_data)\n",
    "\n",
    "print(f\"Combined dataset shape: {all_sampled_data.shape}\")\n",
    "print(\"Saved to '512x384_all_unc.npy'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (1336, 20)\n",
      "Saved to '256x256_all_unc.npy'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# List of input file names\n",
    "file_names = [\n",
    "    '256x256_avg_unc.npy',\n",
    "    '256x256_gau_unc.npy',\n",
    "    '256x256_jpeg_90.npy',\n",
    "    '256x256_res_unc.npy'\n",
    "]\n",
    "\n",
    "# Container for sampled data\n",
    "sampled_data = []\n",
    "\n",
    "# Process each file\n",
    "for file in file_names:\n",
    "    if not os.path.exists(file):\n",
    "        raise FileNotFoundError(f\"File not found: {file}\")\n",
    "    \n",
    "    data = np.load(file)\n",
    "    total_samples = data.shape[0]\n",
    "    sample_size = total_samples // 4  # 25%\n",
    "    \n",
    "    # Randomly select 25% of the data\n",
    "    indices = np.random.choice(total_samples, sample_size, replace=False)\n",
    "    sampled = data[indices]\n",
    "    \n",
    "    sampled_data.append(sampled)\n",
    "\n",
    "# Concatenate all sampled data\n",
    "all_sampled_data = np.concatenate(sampled_data, axis=0)\n",
    "\n",
    "# Save to new file\n",
    "np.save('256x256_all_unc.npy', all_sampled_data)\n",
    "\n",
    "print(f\"Combined dataset shape: {all_sampled_data.shape}\")\n",
    "print(\"Saved to '256x256_all_unc.npy'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
