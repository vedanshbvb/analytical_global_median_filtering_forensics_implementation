{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# table-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Table:\n",
      "     mf3 vs avg  mf3 vs gau  mf3 vs jpeg  mf3 vs res  mf5 vs avg  mf5 vs gau  mf5 vs jpeg  mf5 vs res\n",
      "PoE      0.0075      0.0075       0.0090      0.0090      0.0060      0.0060       0.0075      0.0075\n",
      "AUC      0.9952      0.9992       0.9950      0.9958      0.9949      0.9976       0.9941      0.9953\n"
     ]
    }
   ],
   "source": [
    "##############################################################################################\n",
    "#  512 x 384 mf3 mf5 avg gau res jpeg ###############################################################################################\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load features with labels (last column = label)\n",
    "def load_data_with_labels(file_path):\n",
    "    data = np.load(file_path)\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    return X, y\n",
    "\n",
    "# Load all datasets\n",
    "X_orig, y_orig = load_data_with_labels('512x384_orig_unc.npy')\n",
    "X_avg, y_avg = load_data_with_labels('512x384_avg_unc.npy')\n",
    "X_gau, y_gau = load_data_with_labels('512x384_gau_unc.npy')\n",
    "X_jpeg, y_jpeg = load_data_with_labels('512x384_jpeg_90.npy')\n",
    "X_res, y_res = load_data_with_labels('512x384_res_unc.npy')\n",
    "X_mf3, y_mf3 = load_data_with_labels('512x384_mf3_unc.npy')\n",
    "X_mf5, y_mf5 = load_data_with_labels('512x384_mf5_unc.npy')\n",
    "\n",
    "# Split based on original dataset\n",
    "indices = np.arange(len(y_orig))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.25, random_state=42, stratify=y_orig)\n",
    "np.save('test_indices_1.npy', test_indices)\n",
    "\n",
    "# Prepare training data using consistent indices\n",
    "X_train = np.vstack([\n",
    "    X_orig[train_indices], X_avg[train_indices], X_gau[train_indices],\n",
    "    X_jpeg[train_indices], X_res[train_indices], X_mf3[train_indices], X_mf5[train_indices]\n",
    "])\n",
    "y_train = np.hstack([\n",
    "    y_orig[train_indices], y_avg[train_indices], y_gau[train_indices],\n",
    "    y_jpeg[train_indices], y_res[train_indices], y_mf3[train_indices], y_mf5[train_indices]\n",
    "])\n",
    "\n",
    "# Train SVM model\n",
    "model_1 = SVC(kernel='poly', degree=3, C=1.0, probability=True)\n",
    "model_1.fit(X_train, y_train)\n",
    "\n",
    "# Define comparison pairs\n",
    "pairs = [\n",
    "    ('mf3', 'avg'), ('mf3', 'gau'), ('mf3', 'jpeg'), ('mf3', 'res'),\n",
    "    ('mf5', 'avg'), ('mf5', 'gau'), ('mf5', 'jpeg'), ('mf5', 'res')\n",
    "]\n",
    "\n",
    "# Dataset dictionary\n",
    "data_dict = {\n",
    "    'avg': (X_avg, y_avg),\n",
    "    'gau': (X_gau, y_gau),\n",
    "    'jpeg': (X_jpeg, y_jpeg),\n",
    "    'res': (X_res, y_res),\n",
    "    'mf3': (X_mf3, y_mf3),\n",
    "    'mf5': (X_mf5, y_mf5)\n",
    "}\n",
    "\n",
    "# Prepare evaluation\n",
    "poe_row = []\n",
    "auc_row = []\n",
    "\n",
    "# Evaluate each pair\n",
    "for a, b in pairs:\n",
    "    X_a, y_a = data_dict[a]\n",
    "    X_b, y_b = data_dict[b]\n",
    "\n",
    "    # Select test samples using original indices\n",
    "    Xa_test = X_a[test_indices]\n",
    "    Xb_test = X_b[test_indices]\n",
    "\n",
    "    ya_test = np.ones(len(test_indices))  # label 1 for attack a\n",
    "    yb_test = np.zeros(len(test_indices))  # label 0 for attack b\n",
    "\n",
    "    # Combine for evaluation\n",
    "    X_pair = np.vstack([Xa_test, Xb_test])\n",
    "    y_pair = np.hstack([ya_test, yb_test])\n",
    "\n",
    "    y_pred = model_1.predict(X_pair)\n",
    "    y_proba = model_1.predict_proba(X_pair)[:, 1]\n",
    "\n",
    "    poe = 1 - accuracy_score(y_pair, y_pred)\n",
    "    auc = roc_auc_score(y_pair, y_proba)\n",
    "\n",
    "    poe_row.append(poe)\n",
    "    auc_row.append(auc)\n",
    "\n",
    "# Create result table\n",
    "table = pd.DataFrame([poe_row, auc_row],\n",
    "                     index=['PoE', 'AUC'],\n",
    "                     columns=[f'{a} vs {b}' for a, b in pairs])\n",
    "\n",
    "print(\"Evaluation Table:\")\n",
    "print(table.to_string(float_format=\"%.4f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Table on 256x256 data:\n",
      "     mf3 vs avg  mf3 vs gau  mf3 vs jpeg  mf3 vs res  mf5 vs avg  mf5 vs gau  mf5 vs jpeg  mf5 vs res\n",
      "PoE      0.4343      0.4388       0.4239      0.4493      0.4358      0.4403       0.4254      0.4507\n",
      "AUC      0.7454      0.7162       0.7511      0.6087      0.8037      0.7807       0.8181      0.6822\n"
     ]
    }
   ],
   "source": [
    "##############################################################################################\n",
    "#  256 x 256 mf3 mf5 avg gau res jpeg ###############################################################################################\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load features with labels\n",
    "def load_data_with_labels(file_path):\n",
    "    data = np.load(file_path)\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    return X, y\n",
    "\n",
    "# Load 256x256 datasets\n",
    "X_orig_256, y_orig_256 = load_data_with_labels('256x256_orig_unc.npy')\n",
    "X_avg_256, y_avg_256 = load_data_with_labels('256x256_avg_unc.npy')\n",
    "X_gau_256, y_gau_256 = load_data_with_labels('256x256_gau_unc.npy')\n",
    "X_jpeg_256, y_jpeg_256 = load_data_with_labels('256x256_jpeg_90.npy')\n",
    "X_res_256, y_res_256 = load_data_with_labels('256x256_res_unc.npy')\n",
    "X_mf3_256, y_mf3_256 = load_data_with_labels('256x256_mf3_unc.npy')\n",
    "X_mf5_256, y_mf5_256 = load_data_with_labels('256x256_mf5_unc.npy')\n",
    "\n",
    "# Use previously defined `test_indices` (from 512x384_orig_unc)\n",
    "# If running this separately, you need to re-import or re-calculate them\n",
    "# For context:\n",
    "# indices = np.arange(len(y_orig_512))\n",
    "# train_indices, test_indices = train_test_split(indices, test_size=0.25, random_state=42, stratify=y_orig_512)\n",
    "\n",
    "# Replace with appropriate length if you don't have it already\n",
    "# For example purposes, using this (but use your actual test_indices!):\n",
    "test_indices = np.load('test_indices_1.npy')  # load from file if previously saved\n",
    "\n",
    "# Define dataset dictionary for 256x256\n",
    "data_dict_256 = {\n",
    "    'avg': (X_avg_256, y_avg_256),\n",
    "    'gau': (X_gau_256, y_gau_256),\n",
    "    'jpeg': (X_jpeg_256, y_jpeg_256),\n",
    "    'res': (X_res_256, y_res_256),\n",
    "    'mf3': (X_mf3_256, y_mf3_256),\n",
    "    'mf5': (X_mf5_256, y_mf5_256)\n",
    "}\n",
    "\n",
    "# Define the same comparison pairs\n",
    "pairs = [\n",
    "    ('mf3', 'avg'), ('mf3', 'gau'), ('mf3', 'jpeg'), ('mf3', 'res'),\n",
    "    ('mf5', 'avg'), ('mf5', 'gau'), ('mf5', 'jpeg'), ('mf5', 'res')\n",
    "]\n",
    "\n",
    "# Evaluate using the model previously trained on 512x384\n",
    "poe_row = []\n",
    "auc_row = []\n",
    "\n",
    "for a, b in pairs:\n",
    "    X_a, y_a = data_dict_256[a]\n",
    "    X_b, y_b = data_dict_256[b]\n",
    "\n",
    "    Xa_test = X_a[test_indices]\n",
    "    Xb_test = X_b[test_indices]\n",
    "\n",
    "    ya_test = np.ones(len(test_indices))  # attack A\n",
    "    yb_test = np.zeros(len(test_indices))  # attack B\n",
    "\n",
    "    X_pair = np.vstack([Xa_test, Xb_test])\n",
    "    y_pair = np.hstack([ya_test, yb_test])\n",
    "\n",
    "    y_pred = model_1.predict(X_pair)\n",
    "    y_proba = model_1.predict_proba(X_pair)[:, 1]\n",
    "\n",
    "    poe = 1 - accuracy_score(y_pair, y_pred)\n",
    "    auc = roc_auc_score(y_pair, y_proba)\n",
    "\n",
    "    poe_row.append(poe)\n",
    "    auc_row.append(auc)\n",
    "\n",
    "# Generate evaluation table\n",
    "table = pd.DataFrame([poe_row, auc_row],\n",
    "                     index=['PoE', 'AUC'],\n",
    "                     columns=[f'{a} vs {b}' for a, b in pairs])\n",
    "\n",
    "print(\"Evaluation Table on 256x256 data:\")\n",
    "print(table.to_string(float_format=\"%.4f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Table:\n",
      "     mf35 vs all\n",
      "PoE       0.0075\n",
      "AUC       0.9942\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "# 512 x 384 mf35 all\n",
    "#####################################################################################################\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load features with labels (last column = label)\n",
    "def load_data_with_labels(file_path):\n",
    "    data = np.load(file_path)\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    return X, y\n",
    "\n",
    "# Load datasets\n",
    "X_all, y_all = load_data_with_labels('512x384_all_unc.npy')\n",
    "X_mf35, y_mf35 = load_data_with_labels('512x384_mf35_unc.npy')\n",
    "\n",
    "# Split data (75% train, 25% test) using consistent indices\n",
    "indices = np.arange(len(y_all))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.25, random_state=42, stratify=y_all)\n",
    "np.save('test_indices_2.npy', test_indices)\n",
    "\n",
    "# Prepare training data\n",
    "X_train = np.vstack([X_all[train_indices], X_mf35[train_indices]])\n",
    "y_train = np.hstack([y_all[train_indices], y_mf35[train_indices]])\n",
    "\n",
    "# Train SVM model\n",
    "model_2 = SVC(kernel='poly', degree=3, C=1.0, probability=True)\n",
    "model_2.fit(X_train, y_train)\n",
    "\n",
    "# Create dataset dictionary for evaluation\n",
    "data_dict = {\n",
    "    'all': (X_all, y_all),\n",
    "    'mf35': (X_mf35, y_mf35)\n",
    "}\n",
    "\n",
    "# Define comparison pairs for evaluation\n",
    "pairs = [\n",
    "    ('mf35', 'all')\n",
    "]\n",
    "\n",
    "# Prepare evaluation\n",
    "poe_row = []\n",
    "auc_row = []\n",
    "\n",
    "# Evaluate each pair\n",
    "for a, b in pairs:\n",
    "    X_a, y_a = data_dict[a]\n",
    "    X_b, y_b = data_dict[b]\n",
    "\n",
    "    Xa_test = X_a[test_indices]\n",
    "    Xb_test = X_b[test_indices]\n",
    "\n",
    "    ya_test = np.ones(len(test_indices))  # label 1 for attack a\n",
    "    yb_test = np.zeros(len(test_indices))  # label 0 for attack b\n",
    "\n",
    "    # Combine for evaluation\n",
    "    X_pair = np.vstack([Xa_test, Xb_test])\n",
    "    y_pair = np.hstack([ya_test, yb_test])\n",
    "\n",
    "    y_pred = model_2.predict(X_pair)\n",
    "    y_proba = model_2.predict_proba(X_pair)[:, 1]\n",
    "\n",
    "    poe = 1 - accuracy_score(y_pair, y_pred)\n",
    "    auc = roc_auc_score(y_pair, y_proba)\n",
    "\n",
    "    poe_row.append(poe)\n",
    "    auc_row.append(auc)\n",
    "\n",
    "# Create result table\n",
    "table = pd.DataFrame([poe_row, auc_row],\n",
    "                     index=['PoE', 'AUC'],\n",
    "                     columns=[f'{a} vs {b}' for a, b in pairs])\n",
    "\n",
    "print(\"Evaluation Table:\")\n",
    "print(table.to_string(float_format=\"%.4f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Table (on 256x256 data):\n",
      "     256_mf35 vs 256_all\n",
      "PoE               0.4222\n",
      "AUC               0.7836\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "# 256 x 256 mf35 all\n",
    "#####################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import pandas as pd\n",
    "import joblib  # optional if you decide to save/load model\n",
    "\n",
    "# Define function again (in case you're running standalone)\n",
    "def load_data_with_labels(file_path):\n",
    "    data = np.load(file_path)\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    return X, y\n",
    "\n",
    "# Load previously used test indices\n",
    "# X_all_512, y_all_512 = load_data_with_labels('512x384_all_unc.npy')\n",
    "# indices = np.arange(len(y_all_512))\n",
    "# _, test_indices = train_test_split(indices, test_size=0.25, random_state=42, stratify=y_all_512)\n",
    "\n",
    "test_indices = np.load('test_indices_2.npy')\n",
    "\n",
    "# Load trained model (if saved to file, otherwise assume `model` is in memory)\n",
    "# model = joblib.load('trained_model_512_poly.pkl')\n",
    "\n",
    "# Load 256x256 datasets\n",
    "X_all_256, y_all_256 = load_data_with_labels('256x256_all_unc.npy')\n",
    "X_mf35_256, y_mf35_256 = load_data_with_labels('256x256_mf35_unc.npy')\n",
    "\n",
    "# Create dataset dictionary\n",
    "data_dict = {\n",
    "    '256_all': (X_all_256, y_all_256),\n",
    "    '256_mf35': (X_mf35_256, y_mf35_256)\n",
    "}\n",
    "\n",
    "# Define comparison pair\n",
    "pairs = [('256_mf35', '256_all')]\n",
    "\n",
    "# Evaluate\n",
    "poe_row = []\n",
    "auc_row = []\n",
    "\n",
    "for a, b in pairs:\n",
    "    X_a, y_a = data_dict[a]\n",
    "    X_b, y_b = data_dict[b]\n",
    "\n",
    "    Xa_test = X_a[test_indices]\n",
    "    Xb_test = X_b[test_indices]\n",
    "\n",
    "    ya_test = np.ones(len(test_indices))  # label 1 for attack a\n",
    "    yb_test = np.zeros(len(test_indices))  # label 0 for attack b\n",
    "\n",
    "    X_pair = np.vstack([Xa_test, Xb_test])\n",
    "    y_pair = np.hstack([ya_test, yb_test])\n",
    "\n",
    "    y_pred = model_2.predict(X_pair)\n",
    "    y_proba = model_2.predict_proba(X_pair)[:, 1]\n",
    "\n",
    "    poe = 1 - accuracy_score(y_pair, y_pred)\n",
    "    auc = roc_auc_score(y_pair, y_proba)\n",
    "\n",
    "    poe_row.append(poe)\n",
    "    auc_row.append(auc)\n",
    "\n",
    "# Create result table\n",
    "table = pd.DataFrame([poe_row, auc_row],\n",
    "                     index=['PoE', 'AUC'],\n",
    "                     columns=[f'{a} vs {b}' for a, b in pairs])\n",
    "\n",
    "print(\"Evaluation Table (on 256x256 data):\")\n",
    "print(table.to_string(float_format=\"%.4f\"))\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# table-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Table on 512x384 - attack vs original:\n",
      "     mf3_90 vs orig  mf5_90 vs orig\n",
      "PoE          0.2791          0.0657\n",
      "AUC          0.9858          0.9885\n"
     ]
    }
   ],
   "source": [
    "#############################################################################################################\n",
    "# 512 x 384 ori mf3q mf5q\n",
    "#############################################################################################################\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load features with labels\n",
    "def load_data_with_labels(file_path):\n",
    "    data = np.load(file_path)\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    return X, y\n",
    "\n",
    "# Load specified 512x384 datasets\n",
    "X_orig, y_orig = load_data_with_labels('512x384_orig_unc.npy')\n",
    "# X_mf3, y_mf3 = load_data_with_labels('512x384_mf3_unc.npy')\n",
    "# X_mf5, y_mf5 = load_data_with_labels('512x384_mf5_unc.npy')\n",
    "X_mf3_90, y_mf3_90 = load_data_with_labels('512x384_mf3_90.npy')\n",
    "X_mf5_90, y_mf5_90 = load_data_with_labels('512x384_mf5_90.npy')\n",
    "\n",
    "# Load saved test indices\n",
    "test_indices = np.load('test_indices_1.npy')\n",
    "\n",
    "# Define dataset dictionary\n",
    "data_dict = {\n",
    "    'orig': (X_orig, y_orig),\n",
    "    # 'mf3': (X_mf3, y_mf3),\n",
    "    # 'mf5': (X_mf5, y_mf5),\n",
    "    'mf3_90': (X_mf3_90, y_mf3_90),\n",
    "    'mf5_90': (X_mf5_90, y_mf5_90)\n",
    "}\n",
    "\n",
    "# Define comparison pairs\n",
    "pairs = [\n",
    "    ('mf3_90', 'orig'), ('mf5_90', 'orig')\n",
    "]\n",
    "\n",
    "# Evaluate using the previously trained SVM model (ensure `model_1` exists in memory)\n",
    "poe_row = []\n",
    "auc_row = []\n",
    "\n",
    "for a, b in pairs:\n",
    "    X_a, y_a = data_dict[a]\n",
    "    X_b, y_b = data_dict[b]\n",
    "\n",
    "    Xa_test = X_a[test_indices]\n",
    "    Xb_test = X_b[test_indices]\n",
    "\n",
    "    ya_test = np.ones(len(test_indices))  # label for attack\n",
    "    yb_test = np.zeros(len(test_indices))  # label for original\n",
    "\n",
    "    X_pair = np.vstack([Xa_test, Xb_test])\n",
    "    y_pair = np.hstack([ya_test, yb_test])\n",
    "\n",
    "    y_pred = model_1.predict(X_pair)\n",
    "    y_proba = model_1.predict_proba(X_pair)[:, 1]\n",
    "\n",
    "    poe = 1 - accuracy_score(y_pair, y_pred)\n",
    "    auc = roc_auc_score(y_pair, y_proba)\n",
    "\n",
    "    poe_row.append(poe)\n",
    "    auc_row.append(auc)\n",
    "\n",
    "# Generate and print results table\n",
    "table = pd.DataFrame([poe_row, auc_row],\n",
    "                     index=['PoE', 'AUC'],\n",
    "                     columns=[f'{a} vs {b}' for a, b in pairs])\n",
    "\n",
    "print(\"Evaluation Table on 512x384 - attack vs original:\")\n",
    "print(table.to_string(float_format=\"%.4f\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Table on 256x256 - attack vs original:\n",
      "     mf3_90 vs orig  mf5_90 vs orig\n",
      "PoE          0.5418          0.5269\n",
      "AUC          0.4513          0.4196\n"
     ]
    }
   ],
   "source": [
    "#############################################################################################################\n",
    "# 256 x 256 ori mf3q mf5q\n",
    "#############################################################################################################\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load features with labels\n",
    "def load_data_with_labels(file_path):\n",
    "    data = np.load(file_path)\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    return X, y\n",
    "\n",
    "# Load specified 512x384 datasets\n",
    "X_orig, y_orig = load_data_with_labels('256x256_orig_unc.npy')\n",
    "# X_mf3, y_mf3 = load_data_with_labels('256x256_mf3_unc.npy')\n",
    "# X_mf5, y_mf5 = load_data_with_labels('256x256_mf5_unc.npy')\n",
    "X_mf3_90, y_mf3_90 = load_data_with_labels('256x256_mf3_90.npy')\n",
    "X_mf5_90, y_mf5_90 = load_data_with_labels('256x256_mf5_90.npy')\n",
    "\n",
    "# Load saved test indices\n",
    "test_indices = np.load('test_indices_1.npy')\n",
    "\n",
    "# Define dataset dictionary\n",
    "data_dict = {\n",
    "    'orig': (X_orig, y_orig),\n",
    "    # 'mf3': (X_mf3, y_mf3),\n",
    "    # 'mf5': (X_mf5, y_mf5),\n",
    "    'mf3_90': (X_mf3_90, y_mf3_90),\n",
    "    'mf5_90': (X_mf5_90, y_mf5_90)\n",
    "}\n",
    "\n",
    "# Define comparison pairs\n",
    "pairs = [\n",
    "    ('mf3_90', 'orig'), ('mf5_90', 'orig')\n",
    "]\n",
    "\n",
    "# Evaluate using the previously trained SVM model (ensure `model_1` exists in memory)\n",
    "poe_row = []\n",
    "auc_row = []\n",
    "\n",
    "for a, b in pairs:\n",
    "    X_a, y_a = data_dict[a]\n",
    "    X_b, y_b = data_dict[b]\n",
    "\n",
    "    Xa_test = X_a[test_indices]\n",
    "    Xb_test = X_b[test_indices]\n",
    "\n",
    "    ya_test = np.ones(len(test_indices))  # label for attack\n",
    "    yb_test = np.zeros(len(test_indices))  # label for original\n",
    "\n",
    "    X_pair = np.vstack([Xa_test, Xb_test])\n",
    "    y_pair = np.hstack([ya_test, yb_test])\n",
    "\n",
    "    y_pred = model_1.predict(X_pair)\n",
    "    y_proba = model_1.predict_proba(X_pair)[:, 1]\n",
    "\n",
    "    poe = 1 - accuracy_score(y_pair, y_pred)\n",
    "    auc = roc_auc_score(y_pair, y_proba)\n",
    "\n",
    "    poe_row.append(poe)\n",
    "    auc_row.append(auc)\n",
    "\n",
    "# Generate and print results table\n",
    "table = pd.DataFrame([poe_row, auc_row],\n",
    "                     index=['PoE', 'AUC'],\n",
    "                     columns=[f'{a} vs {b}' for a, b in pairs])\n",
    "\n",
    "print(\"Evaluation Table on 256x256 - attack vs original:\")\n",
    "print(table.to_string(float_format=\"%.4f\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# table-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Table on 512x384 data:\n",
      "     mf3_90 vs avg  mf3_90 vs gau  mf3_90 vs jpeg  mf3_90 vs res  mf5_90 vs avg  mf5_90 vs gau  mf5_90 vs jpeg  mf5_90 vs res\n",
      "PoE         0.2776         0.2776          0.2791         0.2791         0.0642         0.0642          0.0657         0.0657\n",
      "AUC         0.9579         0.9908          0.9647         0.9883         0.9848         0.9929          0.9854         0.9898\n"
     ]
    }
   ],
   "source": [
    "##############################################################################################\n",
    "#  512 x 384 mf3_90 mf5_90 avg gau res jpeg ###############################################################################################\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load features with labels\n",
    "def load_data_with_labels(file_path):\n",
    "    data = np.load(file_path)\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    return X, y\n",
    "\n",
    "# Load 256x256 datasets\n",
    "X_orig_512, y_orig_512 = load_data_with_labels('512x384_orig_unc.npy')\n",
    "X_avg_512, y_avg_512 = load_data_with_labels('512x384_avg_unc.npy')\n",
    "X_gau_512, y_gau_512 = load_data_with_labels('512x384_gau_unc.npy')\n",
    "X_jpeg_512, y_jpeg_512 = load_data_with_labels('512x384_jpeg_90.npy')\n",
    "X_res_512, y_res_512 = load_data_with_labels('512x384_res_unc.npy')\n",
    "X_mf3_512, y_mf3_512 = load_data_with_labels('512x384_mf3_90.npy')\n",
    "X_mf5_512, y_mf5_512 = load_data_with_labels('512x384_mf5_90.npy')\n",
    "\n",
    "# Use previously defined `test_indices` (from 512x384_orig_unc)\n",
    "# If running this separately, you need to re-import or re-calculate them\n",
    "# For context:\n",
    "# indices = np.arange(len(y_orig_512))\n",
    "# train_indices, test_indices = train_test_split(indices, test_size=0.25, random_state=42, stratify=y_orig_512)\n",
    "\n",
    "# Replace with appropriate length if you don't have it already\n",
    "# For example purposes, using this (but use your actual test_indices!):\n",
    "test_indices = np.load('test_indices_1.npy')  # load from file if previously saved\n",
    "\n",
    "# Define dataset dictionary for 256x256\n",
    "data_dict_512 = {\n",
    "    'avg': (X_avg_512, y_avg_512),\n",
    "    'gau': (X_gau_512, y_gau_512),\n",
    "    'jpeg': (X_jpeg_512, y_jpeg_512),\n",
    "    'res': (X_res_512, y_res_512),\n",
    "    'mf3_90': (X_mf3_512, y_mf3_512),\n",
    "    'mf5_90': (X_mf5_512, y_mf5_512)\n",
    "}\n",
    "\n",
    "# Define the same comparison pairs\n",
    "pairs = [\n",
    "    ('mf3_90', 'avg'), ('mf3_90', 'gau'), ('mf3_90', 'jpeg'), ('mf3_90', 'res'),\n",
    "    ('mf5_90', 'avg'), ('mf5_90', 'gau'), ('mf5_90', 'jpeg'), ('mf5_90', 'res')\n",
    "]\n",
    "\n",
    "# Evaluate using the model previously trained on 512x384\n",
    "poe_row = []\n",
    "auc_row = []\n",
    "\n",
    "for a, b in pairs:\n",
    "    X_a, y_a = data_dict_512[a]\n",
    "    X_b, y_b = data_dict_512[b]\n",
    "\n",
    "    Xa_test = X_a[test_indices]\n",
    "    Xb_test = X_b[test_indices]\n",
    "\n",
    "    ya_test = np.ones(len(test_indices))  # attack A\n",
    "    yb_test = np.zeros(len(test_indices))  # attack B\n",
    "\n",
    "    X_pair = np.vstack([Xa_test, Xb_test])\n",
    "    y_pair = np.hstack([ya_test, yb_test])\n",
    "\n",
    "    y_pred = model_1.predict(X_pair)\n",
    "    y_proba = model_1.predict_proba(X_pair)[:, 1]\n",
    "\n",
    "    poe = 1 - accuracy_score(y_pair, y_pred)\n",
    "    auc = roc_auc_score(y_pair, y_proba)\n",
    "\n",
    "    poe_row.append(poe)\n",
    "    auc_row.append(auc)\n",
    "\n",
    "# Generate evaluation table\n",
    "table = pd.DataFrame([poe_row, auc_row],\n",
    "                     index=['PoE', 'AUC'],\n",
    "                     columns=[f'{a} vs {b}' for a, b in pairs])\n",
    "\n",
    "print(\"Evaluation Table on 512x384 data:\")\n",
    "print(table.to_string(float_format=\"%.4f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Table on 256x256 data:\n",
      "     mf3_90 vs avg  mf3_90 vs gau  mf3_90 vs jpeg  mf3_90 vs res  mf5_90 vs avg  mf5_90 vs gau  mf5_90 vs jpeg  mf5_90 vs res\n",
      "PoE         0.5015         0.5060          0.4910         0.5164         0.4866         0.4910          0.4761         0.5015\n",
      "AUC         0.6516         0.6179          0.6557         0.4902         0.6056         0.5743          0.6087         0.4542\n"
     ]
    }
   ],
   "source": [
    "##############################################################################################\n",
    "#  256 x 256 mf3_90 mf5_90 avg gau res jpeg ###############################################################################################\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load features with labels\n",
    "def load_data_with_labels(file_path):\n",
    "    data = np.load(file_path)\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    return X, y\n",
    "\n",
    "# Load 256x256 datasets\n",
    "X_orig_256, y_orig_256 = load_data_with_labels('256x256_orig_unc.npy')\n",
    "X_avg_256, y_avg_256 = load_data_with_labels('256x256_avg_unc.npy')\n",
    "X_gau_256, y_gau_256 = load_data_with_labels('256x256_gau_unc.npy')\n",
    "X_jpeg_256, y_jpeg_256 = load_data_with_labels('256x256_jpeg_90.npy')\n",
    "X_res_256, y_res_256 = load_data_with_labels('256x256_res_unc.npy')\n",
    "X_mf3_256, y_mf3_256 = load_data_with_labels('256x256_mf3_90.npy')\n",
    "X_mf5_256, y_mf5_256 = load_data_with_labels('256x256_mf5_90.npy')\n",
    "\n",
    "# Use previously defined `test_indices` (from 512x384_orig_unc)\n",
    "# If running this separately, you need to re-import or re-calculate them\n",
    "# For context:\n",
    "# indices = np.arange(len(y_orig_512))\n",
    "# train_indices, test_indices = train_test_split(indices, test_size=0.25, random_state=42, stratify=y_orig_512)\n",
    "\n",
    "# Replace with appropriate length if you don't have it already\n",
    "# For example purposes, using this (but use your actual test_indices!):\n",
    "test_indices = np.load('test_indices_1.npy')  # load from file if previously saved\n",
    "\n",
    "# Define dataset dictionary for 256x256\n",
    "data_dict_256 = {\n",
    "    'avg': (X_avg_256, y_avg_256),\n",
    "    'gau': (X_gau_256, y_gau_256),\n",
    "    'jpeg': (X_jpeg_256, y_jpeg_256),\n",
    "    'res': (X_res_256, y_res_256),\n",
    "    'mf3_90': (X_mf3_256, y_mf3_256),\n",
    "    'mf5_90': (X_mf5_256, y_mf5_256)\n",
    "}\n",
    "\n",
    "# Define the same comparison pairs\n",
    "pairs = [\n",
    "    ('mf3_90', 'avg'), ('mf3_90', 'gau'), ('mf3_90', 'jpeg'), ('mf3_90', 'res'),\n",
    "    ('mf5_90', 'avg'), ('mf5_90', 'gau'), ('mf5_90', 'jpeg'), ('mf5_90', 'res')\n",
    "]\n",
    "\n",
    "# Evaluate using the model previously trained on 512x384\n",
    "poe_row = []\n",
    "auc_row = []\n",
    "\n",
    "for a, b in pairs:\n",
    "    X_a, y_a = data_dict_256[a]\n",
    "    X_b, y_b = data_dict_256[b]\n",
    "\n",
    "    Xa_test = X_a[test_indices]\n",
    "    Xb_test = X_b[test_indices]\n",
    "\n",
    "    ya_test = np.ones(len(test_indices))  # attack A\n",
    "    yb_test = np.zeros(len(test_indices))  # attack B\n",
    "\n",
    "    X_pair = np.vstack([Xa_test, Xb_test])\n",
    "    y_pair = np.hstack([ya_test, yb_test])\n",
    "\n",
    "    y_pred = model_1.predict(X_pair)\n",
    "    y_proba = model_1.predict_proba(X_pair)[:, 1]\n",
    "\n",
    "    poe = 1 - accuracy_score(y_pair, y_pred)\n",
    "    auc = roc_auc_score(y_pair, y_proba)\n",
    "\n",
    "    poe_row.append(poe)\n",
    "    auc_row.append(auc)\n",
    "\n",
    "# Generate evaluation table\n",
    "table = pd.DataFrame([poe_row, auc_row],\n",
    "                     index=['PoE', 'AUC'],\n",
    "                     columns=[f'{a} vs {b}' for a, b in pairs])\n",
    "\n",
    "print(\"Evaluation Table on 256x256 data:\")\n",
    "print(table.to_string(float_format=\"%.4f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Table (on 512x384 data):\n",
      "     512_mf35_90 vs 512_all\n",
      "PoE                  0.0479\n",
      "AUC                  0.9890\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "# 512 x384 mf35_90 all\n",
    "#####################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import pandas as pd\n",
    "import joblib  # optional if you decide to save/load model\n",
    "\n",
    "# Define function again (in case you're running standalone)\n",
    "def load_data_with_labels(file_path):\n",
    "    data = np.load(file_path)\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    return X, y\n",
    "\n",
    "# Load previously used test indices\n",
    "# X_all_512, y_all_512 = load_data_with_labels('512x384_all_unc.npy')\n",
    "# indices = np.arange(len(y_all_512))\n",
    "# _, test_indices = train_test_split(indices, test_size=0.25, random_state=42, stratify=y_all_512)\n",
    "\n",
    "test_indices = np.load('test_indices_2.npy')\n",
    "\n",
    "# Load trained model (if saved to file, otherwise assume `model` is in memory)\n",
    "# model = joblib.load('trained_model_512_poly.pkl')\n",
    "\n",
    "# Load 256x256 datasets\n",
    "X_all_512, y_all_512 = load_data_with_labels('512x384_all_unc.npy')\n",
    "X_mf35_512, y_mf35_512 = load_data_with_labels('512x384_mf35_90.npy')\n",
    "\n",
    "# Create dataset dictionary\n",
    "data_dict = {\n",
    "    '512_all': (X_all_512, y_all_512),\n",
    "    '512_mf35_90': (X_mf35_512, y_mf35_512)\n",
    "}\n",
    "\n",
    "# Define comparison pair\n",
    "pairs = [('512_mf35_90', '512_all')]\n",
    "\n",
    "# Evaluate\n",
    "poe_row = []\n",
    "auc_row = []\n",
    "\n",
    "for a, b in pairs:\n",
    "    X_a, y_a = data_dict[a]\n",
    "    X_b, y_b = data_dict[b]\n",
    "\n",
    "    Xa_test = X_a[test_indices]\n",
    "    Xb_test = X_b[test_indices]\n",
    "\n",
    "    ya_test = np.ones(len(test_indices))  # label 1 for attack a\n",
    "    yb_test = np.zeros(len(test_indices))  # label 0 for attack b\n",
    "\n",
    "    X_pair = np.vstack([Xa_test, Xb_test])\n",
    "    y_pair = np.hstack([ya_test, yb_test])\n",
    "\n",
    "    y_pred = model_2.predict(X_pair)\n",
    "    y_proba = model_2.predict_proba(X_pair)[:, 1]\n",
    "\n",
    "    poe = 1 - accuracy_score(y_pair, y_pred)\n",
    "    auc = roc_auc_score(y_pair, y_proba)\n",
    "\n",
    "    poe_row.append(poe)\n",
    "    auc_row.append(auc)\n",
    "\n",
    "# Create result table\n",
    "table = pd.DataFrame([poe_row, auc_row],\n",
    "                     index=['PoE', 'AUC'],\n",
    "                     columns=[f'{a} vs {b}' for a, b in pairs])\n",
    "\n",
    "print(\"Evaluation Table (on 512x384 data):\")\n",
    "print(table.to_string(float_format=\"%.4f\"))\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Table (on 256x256 data):\n",
      "     256_mf35_90 vs 256_all\n",
      "PoE                  0.4820\n",
      "AUC                  0.5916\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "# 256 x 256 mf35_90 all\n",
    "#####################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import pandas as pd\n",
    "import joblib  # optional if you decide to save/load model\n",
    "\n",
    "# Define function again (in case you're running standalone)\n",
    "def load_data_with_labels(file_path):\n",
    "    data = np.load(file_path)\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    return X, y\n",
    "\n",
    "# Load previously used test indices\n",
    "# X_all_512, y_all_512 = load_data_with_labels('512x384_all_unc.npy')\n",
    "# indices = np.arange(len(y_all_512))\n",
    "# _, test_indices = train_test_split(indices, test_size=0.25, random_state=42, stratify=y_all_512)\n",
    "\n",
    "test_indices = np.load('test_indices_2.npy')\n",
    "\n",
    "# Load trained model (if saved to file, otherwise assume `model` is in memory)\n",
    "# model = joblib.load('trained_model_512_poly.pkl')\n",
    "\n",
    "# Load 256x256 datasets\n",
    "X_all_256, y_all_256 = load_data_with_labels('256x256_all_unc.npy')\n",
    "X_mf35_256, y_mf35_256 = load_data_with_labels('256x256_mf35_90.npy')\n",
    "\n",
    "# Create dataset dictionary\n",
    "data_dict = {\n",
    "    '256_all': (X_all_256, y_all_256),\n",
    "    '256_mf35_90': (X_mf35_256, y_mf35_256)\n",
    "}\n",
    "\n",
    "# Define comparison pair\n",
    "pairs = [('256_mf35_90', '256_all')]\n",
    "\n",
    "# Evaluate\n",
    "poe_row = []\n",
    "auc_row = []\n",
    "\n",
    "for a, b in pairs:\n",
    "    X_a, y_a = data_dict[a]\n",
    "    X_b, y_b = data_dict[b]\n",
    "\n",
    "    Xa_test = X_a[test_indices]\n",
    "    Xb_test = X_b[test_indices]\n",
    "\n",
    "    ya_test = np.ones(len(test_indices))  # label 1 for attack a\n",
    "    yb_test = np.zeros(len(test_indices))  # label 0 for attack b\n",
    "\n",
    "    X_pair = np.vstack([Xa_test, Xb_test])\n",
    "    y_pair = np.hstack([ya_test, yb_test])\n",
    "\n",
    "    y_pred = model_2.predict(X_pair)\n",
    "    y_proba = model_2.predict_proba(X_pair)[:, 1]\n",
    "\n",
    "    poe = 1 - accuracy_score(y_pair, y_pred)\n",
    "    auc = roc_auc_score(y_pair, y_proba)\n",
    "\n",
    "    poe_row.append(poe)\n",
    "    auc_row.append(auc)\n",
    "\n",
    "# Create result table\n",
    "table = pd.DataFrame([poe_row, auc_row],\n",
    "                     index=['PoE', 'AUC'],\n",
    "                     columns=[f'{a} vs {b}' for a, b in pairs])\n",
    "\n",
    "print(\"Evaluation Table (on 256x256 data):\")\n",
    "print(table.to_string(float_format=\"%.4f\"))\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# table-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Table on 512x384 - attack vs original:\n",
      "     mf3_90 vs orig_90  mf5_90 vs orig_90\n",
      "PoE             0.2791             0.0657\n",
      "AUC             0.9647             0.9854\n"
     ]
    }
   ],
   "source": [
    "#############################################################################################################\n",
    "# 512 x 384 ori_90 mf3_90 mf5_90\n",
    "#############################################################################################################\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load features with labels\n",
    "def load_data_with_labels(file_path):\n",
    "    data = np.load(file_path)\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    return X, y\n",
    "\n",
    "# Load specified 512x384 datasets\n",
    "X_orig, y_orig = load_data_with_labels('512x384_jpeg_90.npy')\n",
    "\n",
    "X_mf3_90, y_mf3_90 = load_data_with_labels('512x384_mf3_90.npy')\n",
    "X_mf5_90, y_mf5_90 = load_data_with_labels('512x384_mf5_90.npy')\n",
    "\n",
    "# Load saved test indices\n",
    "test_indices = np.load('test_indices_1.npy')\n",
    "\n",
    "# Define dataset dictionary\n",
    "data_dict = {\n",
    "    'orig_90': (X_orig, y_orig),\n",
    "\n",
    "    'mf3_90': (X_mf3_90, y_mf3_90),\n",
    "    'mf5_90': (X_mf5_90, y_mf5_90)\n",
    "}\n",
    "\n",
    "# Define comparison pairs\n",
    "pairs = [\n",
    "    ('mf3_90', 'orig_90'), ('mf5_90', 'orig_90')\n",
    "]\n",
    "\n",
    "# Evaluate using the previously trained SVM model (ensure `model_1` exists in memory)\n",
    "poe_row = []\n",
    "auc_row = []\n",
    "\n",
    "for a, b in pairs:\n",
    "    X_a, y_a = data_dict[a]\n",
    "    X_b, y_b = data_dict[b]\n",
    "\n",
    "    Xa_test = X_a[test_indices]\n",
    "    Xb_test = X_b[test_indices]\n",
    "\n",
    "    ya_test = np.ones(len(test_indices))  # label for attack\n",
    "    yb_test = np.zeros(len(test_indices))  # label for original\n",
    "\n",
    "    X_pair = np.vstack([Xa_test, Xb_test])\n",
    "    y_pair = np.hstack([ya_test, yb_test])\n",
    "\n",
    "    y_pred = model_1.predict(X_pair)\n",
    "    y_proba = model_1.predict_proba(X_pair)[:, 1]\n",
    "\n",
    "    poe = 1 - accuracy_score(y_pair, y_pred)\n",
    "    auc = roc_auc_score(y_pair, y_proba)\n",
    "\n",
    "    poe_row.append(poe)\n",
    "    auc_row.append(auc)\n",
    "\n",
    "# Generate and print results table\n",
    "table = pd.DataFrame([poe_row, auc_row],\n",
    "                     index=['PoE', 'AUC'],\n",
    "                     columns=[f'{a} vs {b}' for a, b in pairs])\n",
    "\n",
    "print(\"Evaluation Table on 512x384 - attack vs original:\")\n",
    "print(table.to_string(float_format=\"%.4f\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Table on 256x256 - attack vs original:\n",
      "     mf3_90 vs orig_90  mf5_90 vs orig_90\n",
      "PoE             0.4910             0.4761\n",
      "AUC             0.6557             0.6087\n"
     ]
    }
   ],
   "source": [
    "#############################################################################################################\n",
    "# 256 x 256 ori_90 mf3_90 mf5_90\n",
    "#############################################################################################################\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load features with labels\n",
    "def load_data_with_labels(file_path):\n",
    "    data = np.load(file_path)\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    return X, y\n",
    "\n",
    "# Load specified 512x384 datasets\n",
    "X_orig, y_orig = load_data_with_labels('256x256_jpeg_90.npy')\n",
    "# X_mf3, y_mf3 = load_data_with_labels('256x256_mf3_unc.npy')\n",
    "# X_mf5, y_mf5 = load_data_with_labels('256x256_mf5_unc.npy')\n",
    "X_mf3_90, y_mf3_90 = load_data_with_labels('256x256_mf3_90.npy')\n",
    "X_mf5_90, y_mf5_90 = load_data_with_labels('256x256_mf5_90.npy')\n",
    "\n",
    "# Load saved test indices\n",
    "test_indices = np.load('test_indices_1.npy')\n",
    "\n",
    "# Define dataset dictionary\n",
    "data_dict = {\n",
    "    'orig_90': (X_orig, y_orig),\n",
    "    # 'mf3': (X_mf3, y_mf3),\n",
    "    # 'mf5': (X_mf5, y_mf5),\n",
    "    'mf3_90': (X_mf3_90, y_mf3_90),\n",
    "    'mf5_90': (X_mf5_90, y_mf5_90)\n",
    "}\n",
    "\n",
    "# Define comparison pairs\n",
    "pairs = [\n",
    "    ('mf3_90', 'orig_90'), ('mf5_90', 'orig_90')\n",
    "]\n",
    "\n",
    "# Evaluate using the previously trained SVM model (ensure `model_1` exists in memory)\n",
    "poe_row = []\n",
    "auc_row = []\n",
    "\n",
    "for a, b in pairs:\n",
    "    X_a, y_a = data_dict[a]\n",
    "    X_b, y_b = data_dict[b]\n",
    "\n",
    "    Xa_test = X_a[test_indices]\n",
    "    Xb_test = X_b[test_indices]\n",
    "\n",
    "    ya_test = np.ones(len(test_indices))  # label for attack\n",
    "    yb_test = np.zeros(len(test_indices))  # label for original\n",
    "\n",
    "    X_pair = np.vstack([Xa_test, Xb_test])\n",
    "    y_pair = np.hstack([ya_test, yb_test])\n",
    "\n",
    "    y_pred = model_1.predict(X_pair)\n",
    "    y_proba = model_1.predict_proba(X_pair)[:, 1]\n",
    "\n",
    "    poe = 1 - accuracy_score(y_pair, y_pred)\n",
    "    auc = roc_auc_score(y_pair, y_proba)\n",
    "\n",
    "    poe_row.append(poe)\n",
    "    auc_row.append(auc)\n",
    "\n",
    "# Generate and print results table\n",
    "table = pd.DataFrame([poe_row, auc_row],\n",
    "                     index=['PoE', 'AUC'],\n",
    "                     columns=[f'{a} vs {b}' for a, b in pairs])\n",
    "\n",
    "print(\"Evaluation Table on 256x256 - attack vs original:\")\n",
    "print(table.to_string(float_format=\"%.4f\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
